{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1TVGJJpOzPiesLPo46gZNCQLMl-y_QIKe","timestamp":1740409705677}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["#Speech Articulatory Coding (SPARC)\n","\n","This lab provides hands-on experience in using Speech Articulatory Coding (SPARC) model.\n","\n","SPARC is a neural encoding-decoding framework that encodes speech into 14 articulatory features (50Hz) with a disentangled speaker embedding. The individual channel in articulatory features corresponds to a physical articulator on the vocal tract, making the features interpretable, controllable and grounded. The decoder synthesizes those features back to speech audio. Please check the [paper](https://arxiv.org/abs/2406.12998) for detail."],"metadata":{"id":"NCNkC9sk_Od3"}},{"cell_type":"code","source":["# Installation\n","\n","!git clone https://github.com/Berkeley-Speech-Group/Speech-Articulatory-Coding.git"],"metadata":{"id":"uBemLVlk-s7W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install -e /content/Speech-Articulatory-Coding/."],"metadata":{"id":"HEdVxUAD_C-b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import sys\n","sys.path.append(\"/content/Speech-Articulatory-Coding/\")"],"metadata":{"id":"Vohymcz9Bjs5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# audio samples\n","!ls /content/Speech-Articulatory-Coding/sample_audio"],"metadata":{"id":"o5UFPqBE_FFw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sparc import load_model\n","import soundfile as sf\n","import IPython.display as ipd\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import matplotlib\n","\n","color_code = {\"UL\":matplotlib.colors.to_rgb(\"#EE3A5B\"),\n","              \"LL\":matplotlib.colors.to_rgb(\"#FFD155\"),\n","              \"LI\":matplotlib.colors.to_rgb(\"#959595\"),\n","              \"TT\":matplotlib.colors.to_rgb(\"#43B962\"),\n","              \"TB\":matplotlib.colors.to_rgb(\"#C44B9F\"),\n","              \"TD\":matplotlib.colors.to_rgb(\"#0093B7\"),\n","              \"Loudness\":matplotlib.colors.to_rgb(\"#FB754D\"),\n","              \"Pitch\":matplotlib.colors.to_rgb(\"#FB754D\"),}\n","\n","def plot_art(ax, art, gap=5, skip_y=False,color=None,alpha=1.0, lw=2):\n","    yticks=[]\n","    ytick_labels=[]\n","    labels = [\"UL\",\"LL\", \"LI\", \"TT\", \"TB\", \"TD\"]\n","    chidxs = []\n","    EMA_channel_label = ['TDX','TDY','TBX','TBY','TTX','TTY','LIX','LIY','ULX','ULY','LLX','LLY']\n","    channel_label = EMA_channel_label\n","    for l in labels:\n","        chidxs.append(EMA_channel_label.index(l+\"X\"))\n","        chidxs.append(EMA_channel_label.index(l+\"Y\"))\n","\n","\n","    for i,ch_i in enumerate(chidxs):\n","        ch_label = channel_label[ch_i]\n","        ytick_labels.append(ch_label)\n","        if ch_i < 12:\n","            art_name= ch_label[:2]\n","            if color is None:\n","                color = color_code[art_name]\n","            ax.plot(art[:,ch_i]-gap*i,color=color,alpha=alpha,lw=lw)\n","        else:\n","            art_name= ch_label\n","            if color is None:\n","                color = color_code[art_name]\n","            ax.plot(art[:,ch_i]-gap*i,color=color,alpha=alpha,lw=lw)\n","        yticks.append(-gap*i)\n","    if skip_y:\n","        ytick_labels = [\"\"]*len(ytick_labels)\n","\n","    ax.set_yticks(yticks,ytick_labels,fontsize=15)\n","\n","    xticks= np.arange(0,len(art),50)\n","    xtick_labels= [f\"{int(x*20/1000)}\" for x in xticks]\n","    ax.set_xticks(xticks, xtick_labels,fontsize=15)\n","    ax.set_xlabel(\"Time (s)\", fontsize=15)\n","    ax.set_xlim(0,len(art))\n","    return yticks"],"metadata":{"id":"udWpznuiAeJB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["coder = load_model(\"en\", device= \"cuda\", use_penn=True)"],"metadata":{"id":"smb9osRoAFF8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"6F3rk7wYd9Ml"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Resynthesis"],"metadata":{"id":"IsVHU58RCFMT"}},{"cell_type":"code","source":["wav_file = '/content/Speech-Articulatory-Coding/sample_audio/diva_babbling-84.wav'"],"metadata":{"id":"vcSPo2WkAUPa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# EMA channel_label = ['TDX','TDY','TBX','TBY','TTX','TTY','LIX','LIY','ULX','ULY','LLX','LLY']\n","code = coder.encode(wav_file)"],"metadata":{"id":"35GXQd5bCMOp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#shapes\n","for name, values in code.items():\n","  print(f\"{name}: {values.shape}\")"],"metadata":{"id":"0EVYBv-lW425"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig,ax = plt.subplots(1,1, figsize=(7,8))\n","plot_art(ax, code['ema'][:400], color='C0', gap=6, alpha=1, lw=2)\n","plt.show()"],"metadata":{"id":"T9D04A9LG600"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["wav = coder.decode(**code)"],"metadata":{"id":"LTknAg3fDXNA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# resynthesized\n","ipd.display(ipd.Audio(wav, rate=coder.sr))\n"],"metadata":{"id":"Dy_9u_feDhaE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ground truth reference\n","wavt,tsr = sf.read(wav_file)\n","ipd.display(ipd.Audio(wavt, rate=tsr))"],"metadata":{"id":"ReA0L2kwDnUC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Zero-shot Voice Conversion"],"metadata":{"id":"mJTLOktFDth6"}},{"cell_type":"code","source":["src_wav_file = \"/content/Speech-Articulatory-Coding/sample_audio/sample1.wav\"\n","trg_wav_file = \"/content/Speech-Articulatory-Coding/sample_audio/sample2.wav\""],"metadata":{"id":"uolwIkD-Dqjo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# source audio\n","src_wav,sr = sf.read(src_wav_file)\n","ipd.display(ipd.Audio(src_wav, rate=sr))\n","\n","# target audio\n","trg_wav,sr = sf.read(trg_wav_file)\n","ipd.display(ipd.Audio(trg_wav, rate=sr))"],"metadata":{"id":"5KBTwjdbETRl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["converted_wav = coder.convert(src_wav_file, trg_wav_file)"],"metadata":{"id":"CC244MqVD089"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# converted\n","ipd.display(ipd.Audio(converted_wav, rate=coder.sr))"],"metadata":{"id":"N2LfNOP3D2pG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["src_code = coder.encode(src_wav_file)\n","converted_code = coder.encode(converted_wav)"],"metadata":{"id":"UC5kgQ7fEc1l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig,ax = plt.subplots(1,1, figsize=(7,8))\n","plot_art(ax, src_code['ema'][:400], color='C0', gap=6, alpha=1, lw=2)\n","plot_art(ax, converted_code['ema'][:400], color='C3', gap= 6, alpha=0.7, lw=1.5)\n","ax.plot([],[],label=\"Original source\", color='C0', lw=2)\n","ax.plot([],[],label=\"After voice conversion\", color='C3', lw=2,)\n","ax.legend(fontsize=15, handlelength=1.3,loc='lower right',bbox_to_anchor=(1.0,0.98), ncol=2, frameon=False)\n","plt.show()"],"metadata":{"id":"hGRHiCgREfMH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Record Yourself!"],"metadata":{"id":"rUUifmttE4uK"}},{"cell_type":"code","source":["# Helper function for recording from Colab. Adapted from https://colab.research.google.com/drive/14v6co8Ec1JUAVdvuX4D08zxbRM1LmtxQ#scrollTo=9y4bzwMsh05H.\n","# You will need to grant mic permission.\n","from base64 import b64decode\n","\n","RECORD = \"\"\"\n","const sleep  = time => new Promise(resolve => setTimeout(resolve, time))\n","const b2text = blob => new Promise(resolve => {\n","  const reader = new FileReader()\n","  reader.onloadend = e => resolve(e.srcElement.result)\n","  reader.readAsDataURL(blob)\n","})\n","var record = time => new Promise(async resolve => {\n","  stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n","  recorder = new MediaRecorder(stream)\n","  chunks = []\n","  recorder.ondataavailable = e => chunks.push(e.data)\n","  recorder.start()\n","  await sleep(time)\n","  recorder.onstop = async ()=>{\n","    blob = new Blob(chunks)\n","    text = await b2text(blob)\n","    resolve(text)\n","  }\n","  recorder.stop()\n","})\n","\"\"\"\n","\n","def record(sec=5, file_name='recorded_audio.wav'):\n","  try:\n","    from google.colab import output\n","  except ImportError:\n","    print('No possible to import output from google.colab')\n","    return ''\n","  else:\n","    print('Recording')\n","    display(ipd.Javascript(RECORD))\n","    s = output.eval_js('record(%d)' % (sec*1000))\n","    print('Saving to', file_name)\n","    b = b64decode(s.split(',')[1])\n","    with open(file_name, 'wb') as f:\n","      f.write(b)\n","    return file_name"],"metadata":{"id":"fi0PnF-MEm8_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Recording from your laptop for 5 seconds. The file will be saved as 'recorded_audio.wav'.\n","# The file can be found in the navigator at the left panel, under the \"content\" folder.\n","# Download the files if you need, otherwise they will be deleted when the session ends.\n","\n","record(sec=10, file_name='recorded_audio.wav')\n","!ffmpeg -y -i ./recorded_audio.wav recorded_audio.wav"],"metadata":{"id":"YpDbJ40pE9Z4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["code = coder.encode('recorded_audio.wav')"],"metadata":{"id":"a8UU02WME-sg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig,ax = plt.subplots(1,1, figsize=(7,8))\n","plot_art(ax, code['ema'][:400], color='C0', gap=6, alpha=1, lw=2)\n","plt.show()"],"metadata":{"id":"Qjy7oopbFhAG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["wav = coder.decode(**code)"],"metadata":{"id":"ETfMqNRSFuId"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# resynthesized\n","ipd.display(ipd.Audio(wav, rate=coder.sr))\n"],"metadata":{"id":"YC-tmoHdFyI7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ground truth reference\n","wavt,tsr = sf.read('recorded_audio.wav')\n","ipd.display(ipd.Audio(wavt, rate=tsr))"],"metadata":{"id":"__QeOMfAF0xN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"RUvgOTXFF5LJ"},"execution_count":null,"outputs":[]}]}